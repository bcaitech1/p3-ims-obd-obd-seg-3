# Detection Project for NAVER AI BoostCamp 

### ğŸ†Score & Standiing

(Public) 0.6051, 4ë“± 
(Private) 0.4741, 7ë“±

<br/><br/>

# ëª©ì°¨ 

- [File Structure](#file-structure)
- [pipeline](#pipeline)
  - [Augmentation](#augmentation)
  - [Modeling](#Modeling)
  - [Loss](#loss)
  - [SWA](#SWA)
  - [Multiscale](#multiscale)
  - [Ensemble](#ensemble)
- [Results](#results)
- [Environment](#environment)
  - [Hardware](#hardware)
  - [Software](#software)
- [Simple Use](#simple-use)
  - [Requirements](#requirements)
  - [Install packages](#install-packages)
  - [Train](#train)
  - [Inference](#inference)

- [Reference Citation](#reference-citation)

<br/><br/>

## File Structure  

### Input
  
```
/
â”‚ 
â”œâ”€â”€ MainModel
â”‚   â”œâ”€â”€ config
â”‚   â”œâ”€â”€ mmdet
â”‚   â”œâ”€â”€ requirements
â”‚   â”œâ”€â”€ faster_rcnn_train.ipynb
â”‚   â”œâ”€â”€ faster_rcnn_train_kfold.ipynb
â”‚   â”œâ”€â”€ faster_rcnn_train_mosaic.ipynb
â”‚   â”œâ”€â”€ faster_rcnn_train_ensemble.ipynb
â”‚   â”œâ”€â”€ faster_rcnn_train_psuedo.ipynb
â”‚   â”œâ”€â”€ gflv2_train.ipynb
â”‚   â”œâ”€â”€ universenet_train.ipynb
â”‚   â”œâ”€â”€ vfnet_train.ipynb
â”‚   â””â”€â”€ requirements
â”‚
â”œâ”€â”€ SwinModel
â”‚   â”œâ”€â”€ apex
â”‚   â”œâ”€â”€ config
â”‚   â”œâ”€â”€ mmdet
â”‚   â”œâ”€â”€ swin_train.ipynb
â”‚   â””â”€â”€ swin_inference.ipynb
â”‚ 
â”œâ”€â”€ README.md
â”œâ”€â”€ pipeline.png
â”œâ”€â”€ __init__.py
```

<br/><br/>

# pipeline

![pipeline](https://github.com/bcaitech1/p3-ims-obd-obd-seg-3/blob/master/detection/pipeline1.png)

### Data
ë°ì´í„°ëŠ” ìƒìœ„ í´ë” [README](https://github.com/bcaitech1/p3-ims-obd-obd-seg-3/blob/master/README.md)ì— ì •ë¦¬ë˜ì–´ ìˆìŒ.

#### Augmentation
```
bash scripts/colorization.sh
bash scripts/stylize.sh
```
#### Train
```
bash scripts/train_detectors.sh
bash scripts/train_universenet.sh
```
#### Loss
```
bash scripts/colorization.sh
bash scripts/stylize.sh
```
#### SWA
- generalizationì— ê°•í•˜ì—¬ test ì…‹ì—ì„œ í›¨ì”¬ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì¸ë‹¤.
- Faster-Rcnn LB ê¸°ì¤€ 0.02AP ì¦ê°€
- SWAë¥¼ mmdetectionì— ì ìš©í•˜ê¸° ì‰½ê²Œ ë§Œë“¤ì–´ì§„ opensourceë¥¼ ì°¸ê³  : [Link](https://github.com/hyz-xmaster/swa_object_detection)

#### Multiscale
```
bash scripts/colorization.sh
bash scripts/stylize.sh
```
#### Ensemble
```
bash scripts/colorization.sh
bash scripts/stylize.sh
```
#### Submission preparing
```
```

<br/><br/>

## Environment

We trained models on our lab's Linux cluster. The environment listed below reflects a typical software / hardware configuration in this cluster.

#### Hardware:
- CPU: Xeon Gold 5120
- GPU: Tesla V100, P40
- Mem: > 90GB
- Data is stored in remote server stroage.

#### Software:
- System: Ubuntu 18.04.4 LTS with Linux 4.4.0-210-generic kernel.
- Python: 3.7 distributed by Anaconda.
- CUDA: 10.1

<br/><br/>

# Results

## Model

| Method                 | mAP       |  config  |  pretrained 
|------------------------|:---------:|:--------:|:---------:
| Faster RCNN            |  0.44    |  config   |
| augmented + GFL v2 + multi scale train                |  0.49    |  config   |  pretrained 
| vfnet r2 101 + multi scale train                 |  0.5336    |  config   |  pretrained 
| vfnet r2 101 + multi scale train + SWA           |   0.5453    |  config   |  pretrained 
| vfnet r2 101 + multi scale train + SWA + WS + GN            |  0.5445    |  config   |  pretrained 
| augmented + UniverseNet + multi scale train            |  0.5820    |  config   |  pretrained 
| DetectoRS           |  0.4848    |  config   |  pretrained 
| Swin-t(30 epoch)            |  0.54    |  config   |  pretrained 
| DETR            |  0.43    |  config   |  pretrained 
| Emprical Attention            |  0.4805    |  config   |  pretrained 


## NMS(non-maximum suppression)

| nms_score_thr                 | iou_threshold     | F-mAP 
|------------------------|:---------:|:---------:
| 0.00    |  0.40    | 0.4481(ì±„íƒ)    
| 0.04    |  0.50    | 0.4373  
| 0.06    |  0.50    | 0.4351    
| 0.00    |  0.40    | 0.4481    
| 0.00    |  0.35    | 0.4462

## MultiScale

| Model                 | Scale     
|------------------------|:---------:
| GFLv2             |   [1333,960],[1333,800],[1333,480]  
| UniverseNet             |   [1333,960],[1333,800],[1333,480]  
| VFNet            |   [1333,800],[1333,900],[1333,1000]


<br/><br/>

## Reference/ Citation

[1] RetinaFace implementation: [biubug6/Pytorch_Retinaface](https://github.com/biubug6/Pytorch_Retinaface)<br/>
[2] WS-DAN implementation: [GuYuc/WS-DAN.PyTorch](https://github.com/GuYuc/WS-DAN.PyTorch).<br/>
[3] EfficientNet implementation: [lukemelas/EfficientNet-PyTorch](https://github.com/lukemelas/EfficientNet-PyTorch).<br/>
[4] Face alignment code is from: [deepinsight/insightface](https://github.com/deepinsight/insightface/blob/master/common/face_align.py).<br/>
